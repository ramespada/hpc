<h1 id="programación-paralela">Programación paralela</h1>

<blockquote>
  <p>La computación de alto rendimiento implica usar la potencia de cálculo para resolver problemas complejos en ciencia, intengiería y gestión. &lt;200b&gt; Para lograr este objetivo, la computación de alto rendimiento se apoya en tecnologías computacionales como los clusters, los supercomputadores o la computación paralela.</p>
</blockquote>

<h2 id="-qué-es-">¿ Qué es ?</h2>

<ul>
  <li>
    <p><strong>Computación serial</strong>: Tradicionalmente el software se escribía para computación <em>en serie</em>. Esto quiere decir que un problema se descomponía en una secuencia de instrucciones, y cada una de estas era ejecutada secuencialmente (una después de la otra) en un sólo procesador, y por lo tanto en cada instante de tiempo una sola instrucción era ejecutada.</p>
  </li>
  <li>
    <p><strong>Computación paralela</strong>: Es el uso <em>simultanteo</em> de recursos computacionales para resolver un problema. El problema se descompone en partes discretas que pueden ser resueltas concurrentemente. Cada parte es descompuesta en una serie de instrucciones, y cada una de ellas puede ser ejecutada en diferentes procesadores al mismo tiempo. Para ello se require un controlador global que coordine y una todos los procesos que se llevan a cabo de forma separada.</p>
  </li>
</ul>

<p>Para resolver un problema en forma paralela, este debe ser capaz de ser descompuesto en partes que se resuelvan simultaneamente. Los recursos computacionales usados pueden ser computadoras con multiples procesadores (<em>cores/núcleos</em>) ó varias computadoras en red.</p>

<p>Ejemplo de problema paralelo: los partidos de la copa del mundo.</p>

<p><img src="hpc/imgs/qatar2022.png" alt="" /></p>

<p>En cada etapa (octavos, cuartos, semi y final) los partidos pueden ser ejecutados en forma paralela, mientras que la ejecución de etapas debe ser en forma serial. Es decir este problema es serial en el sentido horizontal, y paralelo en el sentido vertical.</p>

<h2 id="por-que-es-importante-aprender-hpc">Por que es importante aprender HPC</h2>

<blockquote>
  <p>Ley de Moore: El número de transistores en un circuito integrado se duplica cada 2 años.</p>
</blockquote>

<p>Sin embargo, el número de ciclos que una CPU puede ejecutar por segundo (<em>clock-speed</em>) no aumenta desde el año 2000 (Intel Pentum 4). Esto se debe a la discipación de calor en los transistores.</p>

<p>Por lo tanto, la única forma de mejorar la performance es desarrollar sistemas multi-core.</p>

<h2 id="concurrencia-vs-paralelismo">Concurrencia vs Paralelismo</h2>

<p>La <strong>concurrencia</strong> es cuando tenemos multiples tareas (<em>tasks</em>) logicamente activas al mismo tiempo. Pero puede que no se ejecuten al mismo tiempo. En cambio el <strong>paralelismo</strong> es un tipo de concurrencia en el cual las distintas tareas estan efectivamente activas al mismo tiempo.</p>

<p><img src="hpc/imgs/parallel_vs_concurrent.png" alt="" /></p>

<h2 id="arquitectura-de-von-neuman">Arquitectura de Von Neuman</h2>

<p>Para entender la computación paralela es necesrio tener algunos conocimientos básicos de como funciona una computadora. 
La mayoría de las computadoras electrónicas siguen la arquitectura de <em>John Von Neuman</em> también conocida como <em>computadora de almacenamiento de programas</em>, en ella tanto los programas, las instrucciones y los datos se guardan en una memoria electronica.</p>

<p>Los componentes de estas computadoras son:</p>
<ul>
  <li>
    <ol>
      <li><strong>Memoria</strong>
        <ul>
          <li>RAM (<em>Random Access Memory</em>) Es una memoria en la que el accesso a cualquier porción de información en ella demora el mismo tiempo (es una idealización).</li>
          <li><em>cache</em> es la parte de la memoria de accesso más inmediato para un procesador.</li>
          <li>Cada espacio en la memoria tiene una dirección identificador llamado <em>memory address</em>.</li>
        </ul>
      </li>
    </ol>
  </li>
  <li>
    <ol>
      <li><strong>Unidad de Control</strong>: se encarga de buscar instrucciones y datos de la memoria, decodificarlos y coordinar operaciones (<em>secuencialmente</em>) para resolver alguna tarea.</li>
    </ol>
  </li>
  <li>
    <ol>
      <li><strong>Unidad Lógica/Aritmética</strong>. ejecuta operaciones lógicas y aritméticas.</li>
    </ol>
  </li>
  <li>
    <ol>
      <li>Dispositivo de <strong>Input/Output</strong>: hace referencia a la interfaz entre el humano (operador) y la computadora.</li>
    </ol>
  </li>
</ul>

<h2 id="tipos-de-paralelismo-taxonomía-de-flynn">Tipos de paralelismo (<em>taxonomía de Flynn</em>)</h2>
<p>Flynn clasificó las computadoras en base a dos características: el <em>paralelismo de procesos</em>  y <em>paralelismo de datos</em>.</p>

<ul>
  <li><strong>SISD</strong> <em>(Single Instruction, Single Data)</em> Es básicamente una computadora serial. Sólo se ejecuta una instrucción a la vez, y por lo tanto sólo una ruta de acceso a los datos es necesaria.</li>
  <li><strong>SIMD</strong>  <em>(Single Instruction, Multiple Data)</em> Todos los procesos ejecutan la misma instrucción en cada ciclo temporal. Y cada unidad de procesamiento puede operar en distintos datos. Sirven para problemas caracterizados por alto grado de regularidad (por ejemplo en procesamiento de imagenes y computación gráfica).</li>
  <li><strong>MISD</strong> <em>(Multiple Instruction, Single Data)</em>  Cada unidad de procesamiento actua de forma independiente por distintas vías de instrucciones. Pero hay un solo canal de datos que alimenta a las unidades de procesamiento. (Esta arquitectura no es muy común).</li>
  <li><strong>MIMD</strong> <em>(Multiple Instruction, Multiple Data)</em>
Cada procesador puede trabajar en una vía de instrucciones distinta, y el acceso a los datos puede ser diferente para cada uno. Es el tipo más común de computadoras paralelas.</li>
</ul>

<h2 id="definiciones-y-conceptos">Definiciones y conceptos:</h2>

<ul>
  <li><em>Core/CPU/Procesador</em> Son las unidades básicas de procesamiento que ejecutan una sola tarea. Las CPUs pueden estar organzadas en uno ó más <em>sockets</em>, cada <em>socket</em> con su propia memoria. Cuando una CPU tiene más de 2 sockets suele haber algun hardware que permita intercambiar memoria entre sockets.</li>
  <li><em>Nodo</em>, básicamente una computadora. Generalmente está compuesto de varias CPUs, y tiene memoria propia.</li>
  <li><em>Cluster</em>, conjunto de nodos interconectados entre sí en red formando una <em>supercomputadora</em>.</li>
  <li><em>Task</em> (tarea), Sección lógica discreta de un trabajo computacional. Es tipicamente un subprograma que es ejecutado por un precesador. Un programa paralelo consiste en multiples tasks corriendo en multiples procesadores.</li>
  <li><em>Pipelining</em> descomponer un <em>task</em> en pasos realizados por distintas CPUS como en una linea de montaje. es un tipo de computación paralela.</li>
  <li><em>Shared Memory</em> arquitectura donde los procesadores tienen acceso directo a una memoria física común.</li>
  <li><em>Symmetric Multi-Processor (SMP)</em>  arquitectura donde los procesadores commparten el mismo <em>address-space</em> y tienen igual acceso a todos los recursos, memoria, discos, etc.</li>
  <li><em>Distributed Memory</em> es una arquitectura donde cada procesador tiene su propia memoria física, y el acceso a la memoria entre CPUs se da a traves de una red.</li>
  <li><em>Comunicación</em> los <em>tasks</em> en paralelo necesitan intercambiar datos, a este intercambio le llamamos comunicación.</li>
  <li><em>Syncronización</em> es la coordinación de <em>tasks</em> paralelos en tempo real, suele involucrar esperar a que terminen otros procesos, lo que hace que el tiempo de ejecución incremente.</li>
  <li><em>Granularidad</em> relación entre tiempo de computación y comunicación.
    <ul>
      <li>Fina: Poco trabajo computacional destinado a comunicación de eventos.</li>
      <li>Gruesa: Alto tiempo destinado a comunicar nodos.</li>
    </ul>
  </li>
  <li><em>sppedup</em> relación entre tiempo de ejecucción de la versión serial y paralela de un mismo programa.</li>
  <li><em>Overhead</em>, El tiempo requrido en coordinar tasks en lugar de estar haciendo trabajo útil. La coordinación puede incluir: iniciar tasks, sincronización, comunicación, uso de librerías especificas para paralelizar, y terminación.</li>
  <li><em>Paralelismos masivo</em> arquitecturas con miles ó millones de unidades de procesamiento.</li>
  <li><em>Escalabilidad</em>, Habilidad de mostrar un incremento proporcional en la velocidad con el incremento de recursos computacionales.</li>
  <li><em>FLOPs</em>: de “Floating Point operation per second” (e.g suma, resta, multiplicacion, division) es una medida de performance.</li>
</ul>

<h2 id="límites-y-costos-de-la-computación-paralela">Límites y costos de la computación paralela</h2>

<h3 id="ley-de-amdahl">Ley de Amdahl</h3>

<p>Establece que la velocidad de un programa paralelo viene dada por:</p>

<center>v= (1 - p)<sup>-1</sup></center>
<p>donde p: fracción de código paralelizable.</p>

<p>Considerando el número de procesadores:</p>

<center> v = (p/n + s)<sup>-1</sup></center>
<p>donde n: nuemero de procesadores, s:fración serial.</p>

<p>Esto muestra que por más que nos forcemos en paralelizar al máximo un programa, este siempre tendrá un asíntota con respecto al numero de procesadores que usemos. Por ejemplo para un código que se puede paralelizar al 95\% nunca vamos a alcanzar una velocidad mayor a 20 veces la velocidad en serial.</p>

<h2 id="arquitecturas-de-memoria">Arquitecturas de Memoria</h2>

<p>Hay dos tipos básicos de arquitecturas de computadoras paralelas (<em>clusters</em>):</p>
<ol>
  <li>Clusters de memoria distribuida.</li>
  <li>Clusters de memoria compartida.</li>
</ol>

<h4 id="distributed-memory"><em>Distributed Memory</em></h4>
<p>En estas los procesadores tienen su propia memoria local. En ellas no existe el concepto de <em>global address space</em> a lo largo de todos los procesadores.
Como cada procesador tiene su propia memoria local, opera de forma independiente. Por lo tanto tampoco aplica el concepto de <em>cache</em> cherency.</p>

<h5 id="ventajas">Ventajas:</h5>
<ul>
  <li>Escalabilidad de la memoria: más procesadores más memoria.</li>
  <li>Cada procesador accede rapidamente a su memoria sin interferencia del resto.</li>
  <li>Menos costosas y modulares.</li>
</ul>

<h5 id="desventajas">Desventajas:</h5>
<ul>
  <li>El programador se tiene que ocupar de la comunicación entre procesadores.</li>
  <li>Puede ser dificil mapear estructras de datos basadas en memoria global a este tipo de organización de memoria.</li>
  <li>Acceso no uniforme a memoria. ya que la memoria de otros nodos tarda más que la local.</li>
</ul>

<h4 id="shared-memory"><em>Shared Memory</em></h4>

<p>En teoría estas arquitecturas permiten que los procesadores accedan a cualquier porción de la memoria libremente. Aunque los procesadores operen independientemente todos comparten los mismos recursos de memoria.</p>

<p>Dentro de las arquitecturas de memoria compartida existen dos tipos básicos:</p>

<ul>
  <li>
    <p><strong>Uniform Memory Access (UMA)</strong> Los procesadores son idénticos, y todos tienen igual acceso y en el mismo tiempo a la memoria.</p>
  </li>
  <li>
    <p><strong>Non-Uniform Memory Access (NUMA)</strong> Comumente formados por varios UMA interconectados. El acceso a memoria no es equitativo entre procesadores, aunque todos tengan acceso.</p>
  </li>
</ul>

<h5 id="ventajas-1">Ventajas:</h5>
<ul>
  <li>Tener un global address-space hace más facil la programación.</li>
  <li>Compartir datos entre tasks es rapido y uniforme deido a la proximidad a las CPUs.</li>
</ul>

<h5 id="desventajas-1">Desventajas:</h5>
<ul>
  <li>Falta de escalabilidad entre memoria y CPUs. Más CPUs puede incrementa el tráfico.</li>
  <li>el programador tiene se tiene que ocupar de la sincronización que aseguren un correcto acceso a la memoria global.</li>
</ul>

<h2 id="modelos-de-programación-paralela">Modelos de programación paralela</h2>

<p>Existen varios modelos de programación paralela en uso:</p>
<ul>
  <li><em>Shared memory</em> (sin threads)
    <ul>
      <li>Es el modelo más sencillo.</li>
      <li>Todos los tasks comparten un mismo <em>address space</em> donde pueden leer y escribir asincronicamente.</li>
      <li>Varios mecanismos como <em>locks</em> y <em>semaforos</em> son utilizados para controlar el acceso a la memoria compartida.</li>
      <li>Ejemplos: <strong>SHMEM</strong></li>
    </ul>
  </li>
  <li><em>Threads</em>
    <ul>
      <li>Es un tipo de programación de memoria compartida.</li>
      <li>Un proceso principal (usualmente conocido como <em>master thread</em>) tiene multiples procesos secundarios (<em>slave threads</em>)</li>
      <li>El <em>master thread</em> es la rama principal del proceso completo (por momentos se puede ejecutar serial).</li>
      <li>Ejemplos: <strong>openMP</strong>, <strong>POSIX</strong></li>
    </ul>
  </li>
  <li><em>Messege Passing</em> (de memoria distribuida)
    <ul>
      <li>Un conjunto de tasks usan su porpia memoria local y varios de ellos pueden ó no residir en la misma maquina.</li>
      <li>El intercambio de datos entre tasks se realiza vía envio y recepción de mensajes.</li>
      <li>La transferencia de datos requiere operaciones coperativas que son realizadas por cada proceso.</li>
      <li>Ejemplos: <strong>MPI</strong></li>
    </ul>
  </li>
  <li><em>Paralelización de datos</em>
    <ul>
      <li>También conocido como <strong>PGAS</strong> (<em>Partitioned Global Address space</em>)</li>
      <li>El <em>address space</em> es tratado globalmente.</li>
      <li>La mayoria del trabajo paralelo se focaliza en realizar operaciones en un conjunto de datos.</li>
      <li>Los datos generalmente se organizan en una estructura común como arrays o cubos.</li>
      <li>Ejemplos: <strong>Coarray</strong> (Fortran)</li>
    </ul>
  </li>
</ul>

<p>A continuación se muestra una estructura típica de cluster y las herramientas que nos permiten interactuar con las distintas partes del mismo
<img src="hpc/imgs/cluster_network.png" alt="" /></p>

<h2 id="diseño-de-programas-paralelos">Diseño de programas paralelos</h2>

<p>Sin duda, el primer paso a desarrollar un programa paralelo es entender en primer lugar el problema que se quiere resolver en paralelo. Y analizar antes si el problema es realmente paralelizable.
Es importante identificar:</p>
<ul>
  <li><em>hotspots</em></li>
  <li><em>cuellos de botella</em> del programa</li>
  <li>inhibidores al paralelismo (el más común es la dependencia de datos).</li>
</ul>

<p>y comparar con otros algoritmos posibles (si existen).</p>

<h3 id="paralelismo-automatico">Paralelismo automatico</h3>
<p>Muchos compiladores cuando analizan el código identifican oportunidades para paralelizar, particularmente en loops. En tal caso uno podría analizar el código e identificar inhibidores e intentar eliminarlos de forma tal que el compilador pueda identificarlo y paralelizar.</p>

<p>De todas formas lo más común es que la paralelización se realice manualmente en base a directivas realizadas por el programador.</p>

<h3 id="partición">Partición</h3>

<p>El primer paso en todo programa paralelo es descomponer el problema en pedazos discretos de trabajo que puede ser distribuido a distintos taks.</p>

<p>Hay dos formas básicas de particionar:</p>
<ul>
  <li>
    <p><em>Descomposición del dominio</em>: Esta asociado a descomponer los datos asociados al problema, y hacer que cada task trabaje en una porción de los datos. Por ejemplo se puede descomponer los datos en <em>bloques</em> ó <em>ciclicamente</em>.</p>
  </li>
  <li>
    <p><em>Descomposición funcional</em>: Este se focaliza en el rol que cumple cada pedazo de código en la resolución de problema global. Por ejemplo en el modelado de la atmósfera se descomponen las rutinas en: dinámica, física (turbulencia, convección, radiación, nubes, etc.), química, océano, etc.</p>
  </li>
</ul>

<h3 id="comunicación">Comunicación</h3>
<p>La comunicación sólo es necesaria cuando los tasks comparten datos. Si el problema a tratar se puede descomponer en muchos pedazos autónomos, entonces no es necesario hacer eso de ella.
Algunos factores a considerar:</p>

<ul>
  <li>Evitar <em>sobre-comunicación</em> (<em>overhead</em>)</li>
  <li><em>Latencia vs Ancho de banda</em>
    <ul>
      <li>Latencia: tiempo que demora enviar un mensaje de 0bytes entre dos tasks (seg).</li>
      <li>Ancho de banda: cantidad de información que puede enviarse por unidad de tiempo (bytes/seg).</li>
    </ul>
  </li>
  <li>Visibilidad de comunicaciones. Es deseable tener información sobre la comunicación entre tasks.</li>
  <li>Comunicación sincrónica vs asincrónica</li>
  <li>Estrategia de comunicación:
    <ul>
      <li><em>Punto a punto</em></li>
      <li><em>Colectiva</em></li>
    </ul>
  </li>
  <li>Eficiencia de comunicación</li>
  <li>Sobrecarga y complejidad</li>
</ul>

<h3 id="sincronización">Sincronización</h3>

<p>Tipos de sincronización:</p>
<ul>
  <li>
    <p><em>Barrier</em> /Barrera: Todos los tasks están involucrados. Cuando el último task llega a la barrera entonces se sincronizan los taks.</p>
  </li>
  <li>
    <p><em>Lock</em>/Semáforo: Puede involucrar cualquier numero de taks. Se usan para serializar/proteger acceso a datos globales. Sólo un task a la vez puede usar el lock.</p>
  </li>
  <li>
    <p>Sincrónica</p>
  </li>
</ul>

<h3 id="dependencia">Dependencia</h3>
<p>Hay <em>dependencia entre instrucciones</em> cuando el orden de ejecución de estos afecta el resultado del programa. También existe la <em>dependencia de datos</em> resulta del uso múltiple de la misma locación de datos por distintos tasks.
La dependencia es importante ya que <strong>es el principal inhibidor de paralelismo</strong>.</p>

<h3 id="balance-de-carga">Balance de carga</h3>
<p>El balance de carga (<em>load balancing</em>) se refiera a la forma en que se distribuye la cantidad de trabajo entre taks de forma tal de reducir la capacidad osciosa.</p>

<p>las estrategias posibles son:</p>
<ul>
  <li>
    <p><em>Distribución equitativa</em></p>
  </li>
  <li><em>Asignación dinámica</em>
    <h3 id="granularidad">Granularidad</h3>
    <p>Como ya se dijo, la granularidad hace referencia a la relación entre computación y comunicación.</p>
  </li>
  <li>Granularidad fina
    <ul>
      <li>Periodos cortos de computación intercaldos con eventos de comunicación.</li>
      <li>Facilita balance de carga.</li>
    </ul>
  </li>
  <li>Granularidad gruesa
    <ul>
      <li>Periodos largos de computos seguidos por sincornización.</li>
      <li>Mayor oportunnidad de incrementar performance.</li>
      <li>Dificil de balancear la carga.</li>
    </ul>
  </li>
</ul>

<h3 id="inputoutput">Input/Output</h3>
<p>Las operaciones I/O son generalmente inhibidores de paralelismo. sin embargo existen sistemas de archivos adaptados a paralelización, por ejemplo: <em>GPFS</em>, <em>Lustre</em>, <em>HDF</em></p>
